# -*- coding: utf-8 -*-
"""VAE.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xu0zM5dpffCCz3_o8afnMvYt1mtcL0y5
"""

import tensorflow as tf
import keras as keras
import matplotlib.pyplot as plt
import numpy as np

"""Data-Set Pre-Processing

"""

(x_train,x_train_label),(x_test,x_test_label) = keras.datasets.mnist.load_data()
x_train = x_train.astype('float32')/255
x_test = x_test.astype('float32')/255
x_train = x_train.reshape((len(x_train),np.prod(x_train.shape[1:])))
x_test = x_test.reshape((len(x_test),np.prod(x_test.shape[1:])))

"""Sampling Proccess(Error as can't pass keratensor to tensorflow)

"""

original_dimension = 784
intermidiate_dimension = 256
latent_dimension = 2

input_layer = keras.Input(shape=(original_dimension,))
encoded = keras.layers.Dense(intermidiate_dimension,activation='relu')(input_layer)
x_mean = keras.layers.Dense(latent_dimension)(encoded)
x_log_var = keras.layers.Dense(latent_dimension)(encoded)
def Sampling(args):
  mean_val, var_val = args
  batch = tf.shape(mean_val)[0]
  dim = tf.shape(mean_val)[1]
  epsilon = tf.keras.backend.random_normal(shape=(batch,dim))
  z = mean_val + tf.exp(0.5*var_val)*epsilon
  return z
z =keras.layers.Lambda(Sampling,output_shape = (latent_dimension,))([x_mean,x_log_var])
decoded = keras.layers.Dense(intermidiate_dimension,activation='relu')(z)
decoded = keras.layers.Dense(original_dimension,activation='sigmoid')(decoded)
vae = keras.Model(input_layer,decoded,name='vae')
reconstruction_loss = original_dimension*keras.losses.binary_crossentropy(input_layer,decoded)
def kl_loss_layer(args):
    x_mean, x_log_var = args
    kl_batch = -0.5 * tf.keras.backend.sum(1 + x_log_var - tf.keras.backend.square(x_mean) - tf.keras.backend.exp(x_log_var), axis=-1)
    return kl_batch
original_dimension = 784
intermidiate_dimension = 256
latent_dimension = 2

input_layer = keras.Input(shape=(original_dimension,))
encoded = keras.layers.Dense(intermidiate_dimension,activation='relu')(input_layer)
x_mean = keras.layers.Dense(latent_dimension)(encoded)
x_log_var = keras.layers.Dense(latent_dimension)(encoded)
def Sampling(args):
  mean_val, var_val = args
  batch = tf.shape(mean_val)[0]
  dim = tf.shape(mean_val)[1]
  epsilon = tf.keras.backend.random_normal(shape=(batch,dim))
  z = mean_val + tf.exp(0.5*var_val)*epsilon
  return z
z =keras.layers.Lambda(Sampling,output_shape = (latent_dimension,))([x_mean,x_log_var])
decoded = keras.layers.Dense(intermidiate_dimension,activation='relu')(z)
decoded = keras.layers.Dense(original_dimension,activation='sigmoid')(decoded)
vae = keras.Model(input_layer,decoded,name='vae')
reconstruction_loss = original_dimension*keras.metrics.binary_crossentropy(input_layer,decoded)
kl_loss =  -0.5 * tf.keras.backend.sum(1 + x_log_var - tf.keras.backend.square(x_mean) -tf.keras.backend.exp(x_log_var), axis=-1)
vae_loss = tf.keras.backend.mean(tf.keras.backend.add(reconstruction_loss, kl_loss))
vae.add_loss(vae_loss)
vae.compile(optimizer='adam')
encoder = keras.Model(input_layer,z,name='encoder')
decoder_input = keras.Input(shape=(latent_dimension,))
decoder_layer= vae.layers[-2](decoder_input)
decoder_layer = vae.layers[-1](decoder_layer)
decoder = keras.Model(decoder_input,decoder_layer,name='decoder')

"""CLASS-WISE IMPLEMENTATION

"""

#---Encoder---
class Encoder(tf.keras.Model):
  def __init__(self,input_dim,intermidiate_dim,latent_dim):
    super().__init__()
    self.dense_layer = keras.layers.Dense(intermidiate_dim,activation='relu')
    self.x_mean = keras.layers.Dense(latent_dim)
    self.x_log_var = keras.layers.Dense(latent_dim)
  def call(self,x):
    encoded = self.dense_layer(x)
    x_mean = self.x_mean(encoded)
    x_log_var = self.x_log_var(encoded)
    epsilon = tf.random.normal(shape = tf.shape(x_mean))
    x  = x_mean + tf.exp(0.5*x_log_var)*epsilon
    return x,x_mean,x_log_var
#---Decoder---
class Decoder(tf.keras.Model):
  def __init__(self,input_dim,intermidiate_dim,latent_dim):
    super().__init__()
    self.dense_layer1 = keras.layers.Dense(intermidiate_dim,activation= 'relu')
    self.dense_layer2 = keras.layers.Dense(input_dim,activation = 'sigmoid')

  def call(self,x):
    decoded = self.dense_layer1(x)
    decoded = self.dense_layer2(decoded)
    return decoded
#--VAE-LOSS--
class VAE(tf.keras.Model):
  def __init__(self,input_dim,intermidiate_dim,latent_dim):
    super().__init__()
    self.encoder = Encoder(input_dim,intermidiate_dim,latent_dim)
    self.decoder = Decoder(input_dim,intermidiate_dim,latent_dim)
    self.orginal_dim = input_dim
  def call(self,x):
    z,x_mean,x_log_var = self.encoder(x)
    reconstruction = self.decoder(z)
    return reconstruction
  def train_step(self,data):
    with tf.GradientTape() as tape:
      z,x_mean,x_log_var = self.encoder(data)
      reconstruction = self.decoder(z)
      reconstruction_loss = tf.reduce_mean(tf.keras.losses.binary_crossentropy(data,reconstruction))
      kl_loss = -0.5 * (1 + x_log_var - tf.square(x_mean) - tf.exp(x_log_var))
      kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss,axis=1))
      beta = 0.0001
      total_loss = reconstruction_loss + beta*kl_loss
    grads = tape.gradient(total_loss,self.trainable_weights)
    self.optimizer.apply_gradients(zip(grads,self.trainable_weights))
    return {"loss": total_loss, "reconstruction_loss": reconstruction_loss, "kl_loss": kl_loss}
  def test_step(self, data):
      x = data  # since your val_data = (x_test, None)
      z, x_mean, x_log_var = self.encoder(x)
      reconstruction = self.decoder(z)
      reconstruction_loss = tf.reduce_mean(tf.keras.losses.binary_crossentropy(x, reconstruction))
      kl_loss = -0.5 * (1 + x_log_var - tf.square(x_mean) - tf.exp(x_log_var))
      kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))
      total_loss = reconstruction_loss + kl_loss
      return {"loss": total_loss, "reconstruction_loss": reconstruction_loss, "kl_loss": kl_loss}

vae = VAE(input_dim=784, intermidiate_dim=256, latent_dim=2)
vae.compile(optimizer=tf.keras.optimizers.Adam())

# Train model
vae.fit(x_train, epochs=30, batch_size=128, validation_data=(x_test, None))

"""VAE-Training

"""

history =vae.fit(x_train,epochs=50,batch_size=128,validation_data=(x_test,None))

#encoded_imgs = vae.encoder.predict(x_test)[1] # Select the x_mean (latent representation)
#decoded_imgs = vae.predict(x_test)
decoded_imgs = vae.predict(x_test)
import matplotlib.pyplot as plt

n = 10
plt.figure(figsize=(20, 4))
for i in range(n):
    ax = plt.subplot(2, n, i + 1)
    plt.imshow(x_test[i].reshape(28, 28))
    plt.gray()
    ax.get_xaxis().set_visible(False)
    ax.get_yaxis().set_visible(False)

    ax = plt.subplot(2, n, i + 1 + n)
    plt.imshow(decoded_imgs[i].reshape(28, 28))
    # Now encoded_imgs[i] is a numpy array, so np.array2string works as intended.
    ax.set_xlabel("{}".format(np.array2string(decoded_imgs[i],max_line_width=10)))
    plt.gray()
    ax.get_yaxis().set_visible(False)
plt.show()